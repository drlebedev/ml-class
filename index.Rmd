---
title: "Activity manner prediction"
output: html_document
author: Kirill Lebedev
---

# Summary

The goal of this research is to explore different machine learning models that could be used to predict weight lifting training quality based on data collected from wearable devices. Initial dataset is available from  http://groupware.les.inf.puc-rio.br/har. You can also find a description of dataset by that link.

# Prerequisites

Let's start with loading some libaries:

```{r echo=FALSE,  warning=FALSE}
library(caret)
library(ggplot2)
```

And now we would download training and tests datasets and load them:

```{r echo=FALSE,  warning=FALSE}
if (!file.exists("training.csv")) {
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "training.csv")
}
if (!file.exists("testing.csv")) {
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "testing.csv")
}
training <- read.csv("training.csv", stringsAsFactors = F)
testing <- read.csv("testing.csv", stringsAsFactors = F)
training$classe = factor(training$classe)
```

# Explaratory data analysis

First lets check what data do we have to train model:

```{r}
str(training)
```

Training dataset consists 160 columns and 19622 columns. Let's check activity types distribution first:

```{r}
qplot(classe, data=training, fill=I("orange"), color=I("orange")) + theme_light()
```

As we see activity types are almost equaly distributed in training data.
Overall feature number is pretty big with some features like timestamp that should not be related to activity claases. Lets try to skip axis specific columns in dataset as well as processed stats like avg and variance. We will use total changes for each wearable as well as axis-independent values for each activities. THe following code would extract column names for predictors we are interested in:

```{r}
colstotal <- colnames(training)[grep("^total", colnames(training))]
cols <- colnames(training)[grep("^[a-z]*_[a-z]*$", colnames(training))][5:16]

```
Now lets plot total predictors correlation plots:
```{r}
featurePlot(training[,colstotal], training$classe, plot="pairs")
```

Lets also plot raw activities predictors correlation plots:
```{r}
featurePlot(training[,cols], training$classe, plot="pairs")
```

As we see from diagrams there is now significant correlation between predictors.

# Model training

We will start training from the model with 4 total predictors and random forests.
```{r}
tc <- trainControl(method = "repeatedcv", number = 10)
model1 <- train(classe ~ ., data = training[,c(colstotal, "classe")], method="rf", trControl = tc, preProc = c("center", "scale"))
model1
```

Next we will use not-agregated predictors and random forests.
```{r}
model2 <- train(classe ~ ., data = training[,c(cols, "classe")], method="rf", trControl = tc, preProc = c("center", "scale"))
model2
```

Also we will train model based on total predictors using boosted trees.
```{r}
model3 <- train(classe ~ ., data = training[,c(colstotal, "classe")], method="bstTree", trControl = tc, preProc = c("center", "scale"))
model3
```

And the las model would use boosted trees and raw preditors.
```{r}
model4 <- train(classe ~ ., data = training[,c(cols, "classe")], method="bstTree", trControl = tc, preProc = c("center", "scale"))
model4
```

# Model training analysis

As we see second model is the best one with more than 99% accuracy in cross-validation. Expected out of sample eror rate should be close to 1% based on cross-validation result. We can see theoretical accuracy dependecy from predictors on the following plot:

```{r}
plot(model2)
```

Let's use the model generated to predict classes fro tests data:

```{r}
predict(model2, newdata=testing[,cols])
```

# Conclusion

Overall perfomance of Random Forests is really good on provided dataset. Prediction accuracy exceeds 99% with only 7 predictors. It means that good prediction quality is achivable with agregated data without detailed axis-specific data. It allows to make model less computationally intensive.